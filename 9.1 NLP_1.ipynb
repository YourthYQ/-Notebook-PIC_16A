{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: The Term-Document Matrix and Topic Modeling\n",
    "\n",
    "## Term-Document Matrix\n",
    "\n",
    "In this course, we've already seen a few examples of working with text. We've used basic string operations and `pandas` `str` operations in order to manipulate text data. Now that we have some array programming and machine learning skills under our belt, we can take our exploration of text data much further. \n",
    "\n",
    "In this lecture, we'll introduce one of the most important constructs for analyzing text data: the [term-document matrix.](https://en.wikipedia.org/wiki/Document-term_matrix)\n",
    "\n",
    "This might sound intimidating, but the idea is very simple. Consider the following three sentences. We regard each of them as a \"document.\"\n",
    "\n",
    "1. I like Harry Potter. \n",
    "2. You like Harry Potter. \n",
    "3. I like Totoro.\n",
    "\n",
    "We can think of the term-document matrix as a data frame with a column for each possible word. In each column, we count up how many times that word appears in document. For example, using the three short \"documents\" above, the term-document matrix is: \n",
    "\n",
    "| document | I | you | like | harry | potter | totoro |\n",
    "|----------|---|-----|------|------|------|--------|\n",
    "| 1        | 1 | 0   | 1    | 1    | 1    | 0      |\n",
    "| 2        | 0 | 1   | 1    | 1    | 1    | 0      |\n",
    "| 3        | 1 | 0   | 1    | 0    | 0    | 1      |\n",
    "\n",
    "This turns out to be an extremely convenient format for working with text data, and we'll see soon how to use it for both sentiment analysis (figuring out how \"positive\" a word or sentence is) and topic modeling (figuring out the main \"ideas\" in a set of documents). \n",
    "\n",
    "If you're very persistent, you would be able to make a term-document matrix using a lot of `for`-loops and basic string operations. However, `scikit-learn` offers a much more convenient approach. In this lecture, we'll see an example of organizing our data and constructing the term-document matrix. In coming lectures, we'll start to use our construction for data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Our data for this lecture is the complete text of the short book *Aliceâ€™s Adventures in Wonderland* by Lewis Carroll. The package `nltk` (Natural Language ToolKit) makes it wonderfully easy to obtain this data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/yourth/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import nltk\n",
    "\n",
    "# only have to do this once\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = gutenberg.raw('carroll-alice.txt')\n",
    "s[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the chapters are demaracted by the all-caps word \"CHAPTER\". So, we can simply split on this word to break the book up into chapters. We need to exclude the very first part of the split, since this isn't a real chapter -- it just contains the title and author information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapters = s.split(\"CHAPTER\")[1:]\n",
    "len(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11452,\n",
       " 10989,\n",
       " 9552,\n",
       " 13871,\n",
       " 11986,\n",
       " 13860,\n",
       " 12688,\n",
       " 13656,\n",
       " 12618,\n",
       " 11528,\n",
       " 10392,\n",
       " 11661]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of characters in each chapter\n",
    "[len(c) for c in chapters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's lots of punctuation and special characters in the text, but we don't have to worry about those this time -- there are built-in functions that will filter these out for us. \n",
    "\n",
    "It's helpful to keep ourselves organized by placing the text of each chapter into a data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"chapter\" : range(1, len(chapters) + 1),\n",
    "    \"text\"    : chapters\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I. Down the Rabbit-Hole\\n\\nAlice was beginnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>II. The Pool of Tears\\n\\n'Curiouser and curio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>III. A Caucus-Race and a Long Tale\\n\\nThey we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>IV. The Rabbit Sends in a Little Bill\\n\\nIt w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>V. Advice from a Caterpillar\\n\\nThe Caterpill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>VI. Pig and Pepper\\n\\nFor a minute or two she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>VII. A Mad Tea-Party\\n\\nThere was a table set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>VIII. The Queen's Croquet-Ground\\n\\nA large r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>IX. The Mock Turtle's Story\\n\\n'You can't thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>X. The Lobster Quadrille\\n\\nThe Mock Turtle s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>XI. Who Stole the Tarts?\\n\\nThe King and Quee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>XII\\n\\n           Alice's Evidence\\n\\n\\n'Here...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chapter                                               text\n",
       "0         1   I. Down the Rabbit-Hole\\n\\nAlice was beginnin...\n",
       "1         2   II. The Pool of Tears\\n\\n'Curiouser and curio...\n",
       "2         3   III. A Caucus-Race and a Long Tale\\n\\nThey we...\n",
       "3         4   IV. The Rabbit Sends in a Little Bill\\n\\nIt w...\n",
       "4         5   V. Advice from a Caterpillar\\n\\nThe Caterpill...\n",
       "5         6   VI. Pig and Pepper\\n\\nFor a minute or two she...\n",
       "6         7   VII. A Mad Tea-Party\\n\\nThere was a table set...\n",
       "7         8   VIII. The Queen's Croquet-Ground\\n\\nA large r...\n",
       "8         9   IX. The Mock Turtle's Story\\n\\n'You can't thi...\n",
       "9        10   X. The Lobster Quadrille\\n\\nThe Mock Turtle s...\n",
       "10       11   XI. Who Stole the Tarts?\\n\\nThe King and Quee...\n",
       "11       12   XII\\n\\n           Alice's Evidence\\n\\n\\n'Here..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to grab the `CountVectorizer` class from the `sklearn.feature_extraction.text` module. This module gives a whole range of tools for turning unstructured text into delicious, quantitative numbers that we can feed into algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a `CountVectorizer` object. This is an object which will construct the term-document matrix for us. As usual, this object accepts various parameters. In this case, I've only specified the use of common English-language \"stop words.\" A stop word is a word that's considered uninteresting for the purposes of natural language processing. For example, \"she,\" \"can\", and \"the\" are common stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the term-document matrix is easy, using the `fit_transform()` method on the appropriate column of `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = vec.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a small hitch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12x2312 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5302 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, we haven't really worked with sparse matrices before. While these are very useful in general, for this course we can just convert it into a regular matrix (i.e. 2d `numpy.array()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = counts.toarray()\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, let's convert it into a `DataFrame` with appropriately labeled columns! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame(counts, columns = vec.get_feature_names_out())\n",
    "count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to be extra-organized, we can now add all this information to our original data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, count_df), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Term-Document Matrix\n",
    "\n",
    "We can now use the Term-Document matrix to check how frequently a given term appears in each chapter of the novel. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['alice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot terms to see how often they appear over time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "for term in ['alice', 'dinah', 'queen', 'hatter']:\n",
    "    ax.plot(df[term], label = term)\n",
    "    \n",
    "ax.set(ylim = (0, None))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Alice is a prominent character throughout the entirety of the book. In contrast, Dinah (Alice's pet cat) only appears in the first half of the book, and the Mad Hatter appears in just a few specific chapters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidebar: Normalization\n",
    "\n",
    "In many applications, it is desirable to use not the raw number of times that a word appears. Instead, various normalizations are possible, each of which provide a quantification of how important a word is within a document. For example, one could compute what proportion of a document is allocated to each word. This approach automatically accounts for the fact that some documents are longer than others. \n",
    "\n",
    "The most popular way to normalize is slightly more mathematically complex: it is called [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). We can compute a tf-idf term-document matrix easily, replacing the `CountVectorizer` above with the `TfidfVectorizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = vec.fit_transform(df['text'])\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entries of `count_df` are no longer integers, but rather floats that estimate a weight for a word within each document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't worry much about the difference between count vectorization and tf-idf vectorization in this course, but feel free to try both when working with models to see whether you can improve your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Let's work through an example of *topic modeling*. The idea of topic modeling is to find \"topics\" in documents that tie together many words. Here are some examples of hypothetical topics that you might find in a newspaper: \n",
    "\n",
    "1. **Finance**: \"dollar\", \"stock\", \"banks\"\n",
    "2. **Politics**: \"party\", \"vote\", \"election\"\n",
    "3. **Sports**: \"team\", \"win\", \"game\"\n",
    "\n",
    "We'll see how to use the term-document matrix, in combination with some nice algorithms from `scikit-learn`, to perform topic modeling. Our overall aim is to get a coarse, topic-level summary of the plot of the short book *Aliceâ€™s Adventures in Wonderland* by Lewis Carroll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "# need to do this once to download the data\n",
    "# nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly review the steps that we took to construct our term-document matrix. First, we used the `gutenberg` module to read in the raw text of the book, and split it into chapters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = gutenberg.raw(\"carroll-alice.txt\")\n",
    "chapters = s.split(\"CHAPTER\")[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we created a nice, tidy data frame in which we stored the complete text of each chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"chapter\" : range(1, len(chapters) + 1),\n",
    "    \"text\" : chapters\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, and this is the complex part, we used the `CountVectorizer` from `sklearn` to construct the term-document matrix. In this example, I've used a few more of the arguments for `CountVectorizer`. In particular, because I'd like to eventually be able to see how topics evolve between chapters, I use the `max_df` argument to specify that I'd like like to include words that appear in at most 50% of the chapters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(max_df = 0.5, min_df = 0.0, stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use this `CountVectorizer` to create the term-document matrix and collect it all as a nice, tidy data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = vec.fit_transform(df['text'])\n",
    "counts = counts.toarray()\n",
    "count_df = pd.DataFrame(counts, columns = vec.get_feature_names_out())\n",
    "df = pd.concat((df, count_df), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On To Topic Modeling\n",
    "\n",
    "Now we are ready to run our model! Topic modeling is an *unsupervised* machine learning framework, which means that there's no set of true labels `y`. So, we just need to create the variables `X`. To do this, we can ignore the `text` and `chapter` columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(['text', 'chapter'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many algorithms for topic modeling. We will use *nonnegative matrix factorization* or NMF for now. As usual, there are three easy steps: \n",
    "\n",
    "1. Import the model we want. \n",
    "2. Initialize an instance of the model. \n",
    "3. Fit the model on data. \n",
    "\n",
    "NMF requires us to specify `n_components`, which is the number of topics to find. Choosing the right number of topics is a bit of an art, but there are also quantitative approaches based on Bayesian statistics that we won't go into here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components = 4, init = \"random\", random_state = 0)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important parts of NMF. First, we have the topics themselves, which are stored in the `components_` attribute of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh, what does that mean? We can think of each component as a collection of **weights** for each word. We can find the most important words in each component by finding the words where the weights are highest within that component. We can do this with a handy function called `np.argsort()`, which tells you which entries of an array are the largest, second largest, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders = np.argsort(model.components_, axis = 1)\n",
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use `numpy` \"fancy\" indexing to arrange the words in the needed orders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "important_words = np.array(X.columns)[orders]\n",
    "important_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's convenient to write a function to automate this for us: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def top_words(X, model, component, num_words):\n",
    "    orders = np.argsort(model.components_, axis = 1)\n",
    "    important_words = np.array(X.columns)[orders]\n",
    "    return important_words[component][-num_words:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_words(X, model, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next important aspect of topic modeling is the assignment of topics per document. This is done via weights. We can access this by using the `transform()` method of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = model.transform(X)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights indicate the relative presence of each topic in each chapter. For example, Topic 2 is highly present in the first six chapters, but then mostly absent for the rest of the book. Topic 3 appears in Chapters 7 and 11, and so on. \n",
    "\n",
    "We can also visualize the same information as a line chart. Let's add as labels some of the top words for each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "for i in range(4):\n",
    "    ax.plot(df['chapter'], weights[:,i], label = top_words(X, model, i, 5))\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 0.65), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot allows us to easily see several major features of the plot of the novel, including the tea party with the March Hare, the Mad Hatter, and the Dormouse (Chapter 7), the crocquet game in the court of the Queen of Hearts (Chapter 8), the appearance of the  Mock Turtle and the Lobster in (Chapters 9 and 10), and the reappearance of many characters in Chapter 11. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
